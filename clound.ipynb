{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"clound.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"veoF7081ST3q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"1e264daa-d829-4dee-88f2-d495f2a4215d","executionInfo":{"status":"ok","timestamp":1571129596076,"user_tz":-480,"elapsed":5938,"user":{"displayName":"chen jiang","photoUrl":"","userId":"17563235988389824821"}}},"source":["2#加载云盘\n","from google.colab import drive\n","drive.mount('/content/drive')\n","#改变工作目录\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/DF/clound/')\n","!ls\n","\n","#过12个小时 空间就会被自动清理\n","n = len(os.listdir('trainImg/'))\n","if n > 10000: \n","    print(\"train有图像{}张\".format(n))\n","else:\n","    os.chdir('/content/drive/My Drive/Colab Notebooks/DF/clound/trainImg')\n","    !ls\n","    !unzip -o Train.zip\n","    os.chdir('/content/drive/My Drive/Colab Notebooks/DF/clound/')\n","\n","# !pip install netron\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","backup\t      lib\t\t  testImg   Train_label.csv\n","clound.ipynb  submit_example.csv  trainImg\n","train有图像10300张\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dmeg_dZRiGG6","colab_type":"code","colab":{}},"source":["# import netron\n","# netron.start('lib/darknet53.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMqqQuJx2hUW","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.train import Features, Feature, Example, Int64List, FloatList, BytesList\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","from PIL import Image\n","import pandas as pd\n","import os\n","import time\n","from functools import wraps\n","\n","def timeit(func):\n","    \"\"\"@装饰器 计算func消耗的时间\"\"\"\n","    def wrapper(*args, **kwargs):\n","        start = time.time()\n","        ret = func(*args, **kwargs)\n","        cost = time.time() - start\n","        print(F'【{func.__name__}】{args[1]} cost {cost:.2f}S')\n","        return ret\n","    return wrapper \n","\n","\n","\"\"\"****************************************\"\"\"\n","\"\"\" 处理图片                 \"\"\"\n","\"\"\"****************************************\"\"\"\n","TRAIN_IMG_DIR = 'trainImg/' #训练图片的目录\n","TEST_IMG_DIR = 'testImg/'  #测试图片的目录\n","TRAIN_LIST_FILE = 'Train_label.csv' #训练图片文件名和label的列表文件\n","X_SHAPE = [416, 416, 3] # 预处理后图像的大小\n","\n","class Pic:\n","    @staticmethod\n","    def read_image(path):\n","        \"\"\"\n","        @@@ 读图片  \n","        :params\n","            path: 文件路径\n","        :return \n","            PIL.Image类型的图片\n","        \"\"\"\n","        try:\n","            img = Image.open(path) # img.mode = 'RGB'\n","        except Exception as e:\n","            print('[path] error...')\n","            raise e\n","        if img.mode != 'RGB':\n","            img = img.convert(\"RGB\") #读取图片的过程中如果遇到非'RGB'就转换格式\n","        return img     \n","\n","    @staticmethod\n","    def show_double_img(before, after, num=3):\n","        \"\"\"\n","        @@@ 并列显示处理前和处理后的图片 用于比较\n","        :param\n","            before: list 处理前PIL.Image \n","            after: list 处理后PIL.Image\n","            num:一次显示几行 \n","        :return\n","            None\n","        \"\"\"\n","        plt.figure(figsize=(14, 10), dpi=75, ) #画布 宽1400 高700\n","        for i in range(len(before)):\n","            p = i % num + 1 #在第几个小图显示\n","            plt.subplot(2, num, p)\n","            plt.imshow(before[i])        \n","            plt.subplot(2, num, p + num)\n","            plt.imshow(after[i])          \n","            if (i+1)%num == 0: \n","                plt.show()            \n","        \n","    @staticmethod\n","    def preprocessing(img):\n","        \"\"\"\n","        @@@ 对图像数据进行预处理\n","        :param\n","            img: Image格式的图片\n","        :return\n","            norm后的array [H,W,3]，值范围[-0.5,-0.5]\n","        \"\"\"\n","        #其他处理。。。\n","\n","        #resize crop变成H*W大小\n","        img = Pic._resize(img)\n","        #PIL Image转换为array\n","        img_array = np.asarray(img, np.float32)\n","\n","        # img_norm = tf.image.per_image_standardization(timg) #tensorflow中对图像标准化预处理的API\n","        img_norm = img_array/255. - 0.5  #[-0.5,0.5]归一化\n","        return img_norm\n","\n","    @staticmethod\n","    def _resize(img, region='big'):\n","        \"\"\"\n","        @@@ 对图像裁切 缩减\n","        :param\n","            img: PIL.Image\n","            region: 裁剪区域 ‘small'取原图最小范围\n","        :return\n","            PIL.Image 预定义大小的图片 \n","        \"\"\"\n","        #四周裁剪掉的百分比(基于裁剪后的尺寸) ‘big'最接近原图\n","        if region == 'small':\n","            left, top, right, bottom = 0.1, 0.08, 0.08, 0.25 \n","        elif region == 'middle':\n","            left, top, right, bottom = 0.05, 0.03, 0.05, 0.12 \n","        elif region == 'big':\n","            left, top, right, bottom = 0.03, 0.02, 0.03, 0.06 \n","        else:\n","            left, top, right, bottom = 0, 0, 0, 0 #不裁边\n","        #最终尺寸\n","        H, W = X_SHAPE[:2]\n","        #resize后的图片尺寸\n","        exH = int(H * (1 + top + bottom)) #高\n","        exW = int(W * (1 + left + right)) #宽\n","        h, w = img.size[::-1]  #原始图片的高 宽\n","        new_size = None #用来缩放的新尺寸（宽高）img.resize用宽在前面\n","        if h > w:  #如果原图宽小\n","            if w > exW:  #图片超大\n","                new_size = (exW, int(h*exW/w)) #宽缩到resize尺寸 高相应缩\n","            elif w < W:  #图片超小\n","                new_size = (W, int(h*W/w))  #宽放大到最终尺寸 高同步放大\n","            else: image = img  #图片不大不小 不变\n","        else: #原图高比较小\n","            if h > exH:  #图片超大\n","                new_size = (int(w*exH/h), exH)\n","            elif h < H: #图片超小\n","                new_size = (int(w*H/h), H)\n","            else: image = img\n","            \n","        if new_size: #改变图片大小\n","            image = img.resize(new_size, Image.ANTIALIAS) #压缩质量 改更好的？ \n","                \n","        #对resize好的图片 进行裁剪，满足预定义尺寸H,W （根据天空的实际图片确定的策略）\n","        h, w = image.size[::-1] #resize后的尺寸\n","        top = int(top / (1+top+bottom) * (h-H)) #上高裁切 \n","        left = int(left / (1+left+right) * (w-W)) #左边裁切\n","        image = image.crop((left, top, left+W, top+H)) #四周裁剪 留下H*W区域\n","        return image\n","\n","    @staticmethod\n","    def get_train_NLs(lkind, ratio=None):\n","        \"\"\"\n","        @@@ 获取train所有的image的列表(name,label)\n","        :param\n","            lkind: 'single'单label, 'multi'多label \n","        :return \n","            dataframe(shuffled)\n","        \"\"\"\n","        df = pd.read_csv(TRAIN_LIST_FILE)\n","        if lkind == 'single':  #获取单label的(图片名和label列表)\n","            NLs = df[df.apply(lambda x: len(x['Code'])<=2, axis=1)].copy()\n","        elif lkind == 'multi':  #多label  \n","            NLs = df[df.apply(lambda x: len(x['Code'])>2, axis=1)].copy()\n","        else:\n","            raise RuntimeError('输入错误 lkind= \"{}\"'.format(lkind) )\n","        #文件名上添加路径\n","        NLs['FileName'] = NLs['FileName'].apply(lambda x: TRAIN_IMG_DIR + x)    \n","        #打乱顺序 frac是要返回的比例 1=100%\n","        NLs = NLs.sample(frac=1).reset_index(drop=True) \n","        if ratio:  #为了应付colab经常删文件，故缩小样本节约时间,正常不要这样操作\n","            return NLs[:int(ratio * len(NLs))]\n","        else: \n","            return NLs\n","\n","    @staticmethod\n","    def get_class_num():\n","        \"\"\"\n","        @@@ 获取单标签的分类总数 也就是最后层神经元数目\n","        :return\n","            int\n","        \"\"\"\n","        NLs = Pic.get_train_NLs('single')\n","        return len(NLs['Code'].unique())\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QqgEiZheKqHZ","colab_type":"code","colab":{}},"source":["\"\"\"****************************************\"\"\"\n","\"\"\" 处理数据                 \"\"\"\n","\"\"\"****************************************\"\"\"\n","NUM_PER_TFRECOARD = 2000 #每个tfrecoard文件包括的图片数\n","CLASS_NUM = Pic.get_class_num()  #单标签 标签类别数目\n","TFR_DIR = 'lib/'  #tfrecoard文件的存放路径\n","#用colab时 每次都重新生成文件 可以考虑缩小样本 0.1=1/10 None=不缩小\n","COLAB_PIC_RATIO = 1 \n","BATCH_SIZE = 24  #批量读取 1bacth多少张照片\n","LABEL_CLASS = {  #train数据集的分类\n","            'xs' : '单label训练集',\n","            'vs' : '单label验证集',\n","            'xm' : '多label训练集',           \n","            'vm' : '多label验证集',    \n","    }\n","STEP_PER_EPOCH = {}  #一个epoch需要多少步 总图片/batch_size\n","\n","class DatasetGenerator:\n","    def __init__(self):\n","        #单label训练集和验证集的文件列表\n","        self.x_single_NLs, self.v_single_NLs = self.__separete(Pic.get_train_NLs('single', COLAB_PIC_RATIO))\n","        #多label训练集和验证集的文件列表\n","        self.x_multi_NLs, self.v_multi_NLs = self.__separete(Pic.get_train_NLs('multi', COLAB_PIC_RATIO))\n","\n","        #检查文件夹的tfrecoard文件，若没有则生成新的\n","        self.check_tfrecoard()\n","        #计算step_per_epoch  fit时候用\n","        self.__calc_step_per_epoch()\n","        #标记（临时策略） 用于_deal_label 区分单label还是多label\n","        self.__now_working_class = None\n","        #标记 用于_parse_function 返回只包括img和label，还是包括全部信息\n","        self.__dataset_for_train = True #True 表示返回训练用的格式(img label)\n","        \n","    def check_tfrecoard(self):\n","        \"\"\"\n","        @@@ 检查文件夹的tfrecoard文件，若没有则生成新的\n","        :param\n","            None\n","        :return \n","            None\n","        \"\"\"       \n","        #类别名  单label训练集和验证集   多label训练集和验证集 \n","        classes = ['xs', 'vs', 'xm', 'vm']\n","        #检查每一个数据集的tfrecoards是否存在\n","        for c in classes:\n","            fnames = self.__get_tfr_file_names(lclass=c) #对应的文件名\n","            # print('{}检查 TFRecoard:{}'.format(c, fnames))\n","            for f in fnames:\n","                if not os.path.isfile(f):\n","                    #只要有一个文件不存在，就全部重新生成\n","                    print('【{}】不存在，\"{}\"所有tfrecoard重新生成[{}个*{}张/个]...'.format(\n","                        f, LABEL_CLASS[c], len(fnames), NUM_PER_TFRECOARD))\n","                    try:\n","                        self._create_class_tfrecoards(c) #单个数据集的tfrecoard全部重新生成\n","                    except Exception as e:\n","                        raise e(f'创建{f}失败！')\n","                    else:\n","                        break #已全部重新生成 后面的文件不用检查了\n","                        \n","            print('对【\"{}\"{}】的{}个TFRecoard，检查完毕！'.format(c, LABEL_CLASS[c], len(fnames)))\n","        print('OK! 【{}】的所有TFRecoard文件检查完毕！\\n'.format(classes))\n","        \n","    def _create_class_tfrecoards(self, lclass):\n","        \"\"\"\n","        @@@ 把当前数据集图片列表 生成tfrecoard文件 \n","        :param\n","            lclass: train数据集的分类\n","        :return \n","            True成功  False失败\n","        \"\"\"     \n","        assert lclass in LABEL_CLASS.keys(), \"lcalss 输入error...\"\n","        #当前数据集下的 图片列表\n","        df = self.__get_NLs_by_class(lclass)\n","        #应该产生的文件名\n","        fnames = self.__get_tfr_file_names(lclass=lclass)\n","        print('\"{}\"：{}张图片 准备create TFRecoard:{}'.format(lclass, len(df), fnames))\n","        \n","        start = 0\n","        i = 0 #文件名序号\n","        while start < len(df):\n","            fname = fnames[i]  #当前写入的文件名\n","            df_slice = df[start:start+NUM_PER_TFRECOARD]\n","            if self.__create_tfrecoard(fname, df_slice):\n","                print('create TFRecoard[{}]  从df[{}:]... Success'.format(fname, start))\n","                i += 1\n","                start += NUM_PER_TFRECOARD\n","            else:\n","                print('create TFRecoard[{}]  从df[{}:]... Faile'.format(fname, start))\n","                return False\n","        return True        \n","\n","    @timeit    \n","    def __create_tfrecoard(self, tfrname, df, option=None):\n","        \"\"\"\n","        @@@ 把输入的图片列表 生成一个tfrecoard文件  若存在则覆盖\n","        :param\n","            tfrname：生成的tfrecoard文件 名\n","            df：要保存的图片 FileName和lable的列表\n","        :return \n","            True成功  False失败\n","        \"\"\"            \n","        print('---- 准备创建tfrecoard文件【{}】...'.format(tfrname, len(df)))\n","        ok_num = 0  #成功写入tfrecoard的图片数目\n","        #考虑用 with tf.io.TFRecordWriter(tfrname, options=option) as writer:\n","        trf_writer = tf.io.TFRecordWriter(tfrname, options=option)\n","        for index, (fname, label) in df.iterrows():\n","            try:\n","                img = Pic.read_image(fname)\n","            except:\n","                print(f'---- {fname}不存在，跳过...')\n","                continue  #文件不存在 忽略 处理下一个\n","            try:\n","                img = Pic.preprocessing(img) #裁剪 归一化等预处理\n","                x_shape = img.shape #处理后的图片尺寸 等于输入到模型中的图片尺寸\n","                assert x_shape == tuple(X_SHAPE), \"准备写入TRF的图片尺寸不等于{}\".format(X_SHAPE)\n","                img = img.reshape(-1) #变成一维\n","                # img_list = img.tolist()  \n","                label = [int(x) for x in label.split(';')]\n","                #对多label的情况，把label固定到5个长度（batch要统一长度）  即一张图片最多5种云\n","                if len(label) > 1:\n","                    label += [0,0,0]\n","                    label = label[:5]\n","                name = [tf.compat.as_bytes(fname)]\n","                #内层feature编码方式\n","                # print('内层feature编码', name, label)\n","                feature_internal = {\n","                            'image_raw' : Feature(float_list = FloatList(value=img)), #图形数据\n","                            'name'  : Feature(bytes_list = BytesList(value=name)), #路径文件名\n","                            'img_shape' : Feature(int64_list = Int64List(value=x_shape)),  \n","                            'label'   : Feature(int64_list = Int64List(value=label)),  #字符串的label   \n","                            }\n","                #使用tf.train.Example将features编码数据封装成特定的PB协议格式\n","                example = Example(features=Features(feature=feature_internal))\n","                #将序列化为字符串的example数据写入协议缓冲区\n","                trf_writer.write(example.SerializeToString())\n","            except:\n","                print(f'---- {fname}图片处理 保存过程中出错.')\n","                return False\n","            else:\n","                ok_num += 1\n","        print(f'---- successfully created [{tfrname}]，共处理图片{ok_num}张')\n","        #关闭TFRecords文件操作接口    \n","        trf_writer.close() \n","        return True\n","        \n","    @timeit    \n","    def read_data_from_TFRecoard(self, lclass, batch_size=BATCH_SIZE):\n","        \"\"\"\n","        @@@ 获取数据集数据 从tfrecoard里读取\n","        :param\n","            lclass: train数据集的分类\n","        :return \n","            dataset  \n","            [batch,H,W,3] if batch， 3D[H,W,3] if no batch\n","            None if no file to read\n","        \"\"\"\n","        assert lclass in LABEL_CLASS.keys(), \"lcalss 输入error...\"\n","        fnames = self.__get_tfr_file_names(lclass=lclass) #文件名列表\n","        self.__now_working_class = lclass  #传递给__deal_label()\n","        \n","        tfr_files = [] #待读取的tfrecoard文件列表\n","        for f in fnames:\n","            if os.path.exists(f): tfr_files.append(f)\n","        if tfr_files == []: return None\n","          \n","        print('从【{}...】读取{}个TFRecoard...'.format(tfr_files[0], len(tfr_files)))\n","        dataset_raw = tf.data.TFRecordDataset(tfr_files)\n","        # Set the number of datapoints you want to load and shuffle \n","        dataset = dataset_raw.shuffle(buffer_size = NUM_PER_TFRECOARD//20)\n","        #To decode the message use the tf.train.Example.FromString method.\n","        #example_proto = tf.train.Example.FromString(serialized_example)\n","        #执行解析函数 得到数据集    \n","        dataset = dataset.map(self.__parse_function)\n","        # 不加参数=无限重复数据集\n","        dataset = dataset.repeat()                   \n","        # 定义batchsize大小\n","        if batch_size: #batch时 必须保证各元素长度都一样\n","            dataset = dataset.batch(batch_size)\n","        return dataset\n","\n","    def iterator_from_tfrecoard(self, lclass, batch_size=BATCH_SIZE):\n","        \"\"\"\n","        @ 获取iterator 数据集数据从tfrecoard里读取\n","        :param\n","            lclass: train数据集的分类\n","        :return \n","            interator 可迭代的image label数据\n","            [batch,H,W,3] if batch， 3D[H,W,3] if no batch\n","            None if no file to read\n","        \"\"\"\n","        dataset = self.read_data_from_TFRecoard(lclass, batch_size)\n","        iterator = dataset.make_one_shot_iterator()\n","        # return iterator\n","        if self.__dataset_for_train:  #训练时 只用（img, label）\n","            img, label = iterator.get_next()\n","            return (img, label)\n","        else:\n","            img, label, name = iterator.get_next()\n","            return (img, label, name)\n","#             \n","#         img = tf.reshape(img, [BATCH_SIZE]+X_SHAPE)\n","#         label = tf.reshape(label, [BATCH_SIZE]+[CLASS_NUM])\n","# \n","#         if self.__dataset_for_train: \n","#             while True:       \n","#                 yield (img, label)\n","#         else:\n","#             while True:\n","#                 yield (img, label, name)\n","\n","\n","    def __parse_function(self, example_proto):\n","        \"\"\"\n","        @@@ 解析函数  解析dataset的每一个Example\n","        :param \n","            example_proto: 单个example序列化后的样本\n","        :return\n","            img: array[H,W,C]\n","            name: b''\n","            label: [int]\n","        \"\"\"\n","        feature_description  = {\n","            'image_raw' : tf.io.VarLenFeature(dtype=tf.float32),\n","            'name' : tf.io.VarLenFeature(dtype=tf.string),\n","            'img_shape' : tf.io.FixedLenFeature(shape=(3,), dtype=tf.int64),\n","            'label' : tf.io.VarLenFeature(dtype=tf.int64)\n","            }\n","        #把序列化样本和解析字典送入函数里得到解析的样本        \n","        parsed_example = tf.io.parse_single_example(example_proto, feature_description ) #返回字典\n","        #解码 \n","        img = tf.sparse.to_dense(parsed_example['image_raw'], default_value=0) # 稀疏->密集表示\n","\n","        #img = tf.reshape(img, parsed_example['img_shape']) # 转换为原来形状\n","        #没搞定 用tf返回 DatasetV1Adapter shapes: (?, ?, ?) 先用固定尺寸来转换\n","        img = tf.reshape(img, X_SHAPE) # 转换为原来形状\n","        label = self.__deal_label(parsed_example['label'])\n","        name = tf.sparse.to_dense(parsed_example['name'], default_value='')\n","        name = tf.squeeze(name,)\n","        #如果有batch 则必须保证各item返回各元素长度都一样，才能加入一个batch内   \n","        if self.__dataset_for_train:\n","            return img, label \n","        else:\n","            return img, label, name    \n","\n","\n","    def __deal_label(self, label):\n","        \"\"\"\n","        @@@ 把原始字符串的label 变成int的list\n","        :param\n","            label：原始label 单label 或者多label(e.g. 1:12)\n","        :return \n","            list[int]  \n","        \"\"\"\n","        lclass = self.__now_working_class  #现在正在处理哪个数据集\n","        assert lclass in LABEL_CLASS.keys(), \"lcalss 输入error...\"\n","        label = tf.sparse.to_dense(label, default_value=0)\n","        if lclass == 'xs' or lclass == 'vs': #如果是单label 直接转换为one-hot\n","            print()\n","            label = tf.one_hot(label[0], CLASS_NUM)\n","        else:  #如果是多label  还没想好\n","            print('现在是多label数据集\"{}\"，暂时不处理label'.format(lclass))\n","        return label       \n","\n","    def __separete(self, df):\n","        \"\"\"\n","        @@@ 把数据集分割成训练集和验证集 \n","        :return\n","            x,v 按比例划分的训练集和验证集\n","        \"\"\"\n","        ratio = 0.7  #训练集的大小 70%\n","        truncation = int(len(df) * ratio) #训练集和验证集分割点\n","        x = df[:truncation] #训练集\n","        v = df[truncation:] #验证集\n","        return x,v\n","    \n","    def __get_NLs_by_class(self, lclass):\n","        \"\"\"\n","        @@@ 通过lclass 获取对应数据集的图片 FileName和lable的列表\n","        :param\n","            lclass: 'xs' 'vs' 单label训练集和验证集 \n","                'xm' 'vm' 多label训练集和验证集 \n","        :return\n","            dataframe\n","        \"\"\"\n","        xs = self.x_single_NLs\n","        vs = self.v_single_NLs\n","        xm = self.x_multi_NLs\n","        vm = self.v_multi_NLs\n","        try:\n","            df = eval(lclass)\n","        except:\n","            raise RuntimeError('输入错误 lclass= \"{}\"'.format(lclass))\n","        return df.reset_index(drop=True)\n","\n","    def __get_tfr_file_names(self, lclass):\n","        \"\"\"\n","        @@@ 获取各种数据集应该产生的tfrecoard文件的列表\n","        :param\n","            lclass: 'xs' 'vs' 单label训练集和验证集 \n","                'xm' 'vm' 多label训练集和验证集 \n","        :return \n","            list tfrecoard文件名列表\n","        \"\"\"\n","        df = self.__get_NLs_by_class(lclass) #图片名列表\n","        file_num = (len(df)-1) // NUM_PER_TFRECOARD + 1\n","\n","        #设置的文件名 (考虑加路径 暂时同一目录下)\n","        names = [TFR_DIR + lclass + '_' + str(x+1) + '.tfrecord' for x in range(file_num)]\n","        return names\n","    \n","    def __calc_step_per_epoch(self):\n","        \"\"\"\n","        @@@ 计算每个label class的一个epoch需要几步能遍历\n","        :param\n","            None\n","        :return \n","            None 保存到\n","        \"\"\"    \n","        lclass = LABEL_CLASS.keys()\n","        for label in lclass:\n","            NLs = self.__get_NLs_by_class(label)\n","            #简单一点 暂时用dataframe的列表，而不是真正的文件里面的图片数，实际上应该差不多\n","            pic_num = len(NLs)  #对应数据集的图片数\n","            STEP_PER_EPOCH[label] = pic_num // BATCH_SIZE + 1\n","        \n","    def __get_clound_name(self, labels):\n","        \"\"\"\n","        @@@ 根据label 获取对应云的名字\n","        :param\n","            labels: int的list\n","        :return \n","            list 云的名字\n","        \"\"\"        \n","        clound = {    #编号（type）  云状类型\n","                0:'', #仅填充用\n","                1:'中云-高积云-絮状高积云',\n","                2:'中云-高积云-透光高积云',\n","                3:'中云-高积云-荚状高积云',\n","                4:'中云-高积云-积云性高积云',\n","                5:'中云-高积云-蔽光高积云',\n","                6:'中云-高积云-堡状高积云',\n","                7:'中云-高层云-透光高层云',\n","                8:'中云-高层云-蔽光高层云',\n","                9:'高云-卷云-伪卷云',\n","                10:'高云-卷云-密卷云',\n","                11:'高云-卷云-毛卷云',\n","                12:'高云-卷云-钩卷云',\n","                13:'高云-卷积云-卷积云',\n","                14:'高云-卷层云-匀卷层云',\n","                15:'高云-卷层云-毛卷层云',\n","                16:'低云-雨层云-雨层云',\n","                17:'低云-雨层云-碎雨云',\n","                18:'低云-积云-碎积云',\n","                19:'低云-积云-浓积云',\n","                20:'低云-积云-淡积云',\n","                21:'低云-积雨云-鬃积雨云',\n","                22:'低云-积雨云-秃积雨云',\n","                23:'低云-层云-碎层云',\n","                24:'低云-层云-层云',\n","                25:'低云-层积云-透光层积云',\n","                26:'低云-层积云-荚状层积云',\n","                27:'低云-层积云-积云性层积云',\n","                28:'低云-层积云-蔽光层积云',\n","                29:'低云-层积云-堡状层积云',\n","                }\n","        names = []\n","        for label in labels:\n","            names.append(clound[label])\n","        return names\n","\n","    def _test_read_write(self):\n","        \"\"\"@测试整个流程\"\"\"\n","        sess =  tf.compat.v1.InteractiveSession() \n","        sess.run(tf.compat.v1.global_variables_initializer())\n","        \n","        self.__dataset_for_train = False #flag dataset读取全部类别\n","        classes = ['xs', 'vs', 'xm', 'vm'][3:]\n","        for c in classes:\n","            print('测试{} \"{}\"tfrecoard...单文件最多{}张图片  BATCH：{}'.format(\n","                c, LABEL_CLASS[c], NUM_PER_TFRECOARD, BATCH_SIZE))\n","            dataset = dg.read_data_from_TFRecoard(c)\n","            iter = tf.compat.v1.data.make_one_shot_iterator(dataset)\n","            imgs, labels, names = iter.get_next()\n","            # self.__test_print(imgs, labels, names)\n","\n","    def _test_resize(self):\n","        \"\"\"@测试 resize函数\"\"\" \n","        before, after = [], [] \n","        df = self.__get_NLs_by_class('vm')[:110]\n","        for index, (fname, label) in df.iterrows():\n","            img = Pic.read_image(fname)\n","            before.append(img)\n","            ret = Pic._resize(img, region='small')\n","            after.append(ret)\n","        Pic.show_double_img(before, after)\n","\n","\n","    def __test_print(self, imgs, labels, names): \n","        \"\"\"@测试 显示iterator的元素\"\"\"         \n","        p_freq = {} #打印频率 \n","        try:\n","            for i in range(2):\n","                image = imgs.eval()+0.5\n","                name = names.eval()\n","                label = labels.eval()\n","                print('  batch {}: '.format(i+1))\n","                for j in range(image.shape[0])[:]:\n","                    p_freq[name[j]] = p_freq.get(name[j], 0) + 1\n","                    print('  No.{} image name:{}  label:{} '.format(j+1, name[j], label[j]  ))\n","                    # xname = name[j].decode('utf-8')  #图片上显示的xlable名字\n","                    # for n in self.__get_clound_name(label[j]):\n","                    #     xname += \" \" + n   \n","                    # plt.xlabel(xname, fontproperties=\"SimHei\")\n","                    plt.imshow(image[j])\n","                    plt.show()\n","        except tf.errors.OutOfRangeError:\n","            print(\"end!读完了...\")        \n","\n","        print('{}张图片调用频率：\\n'.format(len(p_freq)))\n","        for key, value in p_freq.items():\n","            print('{value} : {key}'.format(key = key, value = value))\n","        \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcSjHGxk5WaC","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, AveragePooling2D, Dropout, Concatenate, Add, ZeroPadding2D, UpSampling2D\n","from tensorflow.keras.layers import Flatten, BatchNormalization, Activation, Input, SeparableConv2D, LeakyReLU\n","from tensorflow.keras.models import Model, Sequential, load_model \n","from tensorflow.keras import regularizers\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","# weights_kernel = os.path.join(TFR_DIR, 'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n","\n","EPOCHS = 50  #fit训练次数\n","\n","def showinfo(func):\n","    \"\"\"@装饰器 显示辅助信息 shape param等\"\"\"\n","    def wrapper(*args, **kwargs):\n","        # print('调用', func.__name__, args[1], 'args:',args, 'kw:',kwargs)\n","        res = func(*args, **kwargs)\n","        return res\n","    return wrapper\n","\n","class MyModel():\n","    def __init__(self):\n","        self.model = self.create_model() \n","        #checkpoint保存的文件路径\n","        self.cp_path = 'backup/weights-{epoch:02d}-{val_acc:.3f}.hdf5' \n","        self.cp_dir = os.path.dirname(self.cp_path)\n","\n","    \"\"\"\n","    model的组件\n","    \"\"\"\n","    @showinfo\n","    @wraps(Conv2D)\n","    def __Conv2D(self, x, *args, **kwargs):\n","        \"\"\"\n","        @ 2D卷积层\n","        :param\n","            x: input \n","            args[0]: Conv2D filters\n","            args[1]: Conv2D kernels\n","            kwargs: 可选 strides ...\n","        :return \n","            Output tensor after applying `Conv2D` \n","        \"\"\" \n","        conv_kwargs = {'kernel_regularizer': regularizers.l2(5e-4)} #加点正则化 \n","        #'strides'==(2,2) 相当于pool\n","        conv_kwargs['padding'] = 'valid' if kwargs.get('strides')==(2,2) else 'same' #最好还是传'padding'\n","        conv_kwargs.update(kwargs)\n","        return Conv2D(*args, **conv_kwargs)(x)\n","\n","    @showinfo\n","    def __Conv2D_BN(self, x, *args, **kwargs):\n","        \"\"\"\n","        @ 2D卷积层 + batch-normalization + LeakyReLU\n","        :param\n","            x: input \n","            args[0]: Conv2D filters\n","            args[1]: Conv2D kernels\n","        :return \n","            Output tensor after applying `Conv2D` and `BatchNormalization`\n","        \"\"\"  \n","        no_bias_kwargs = {'use_bias': False}\n","        no_bias_kwargs.update(kwargs)\n","        x = self.__Conv2D(x, *args, **no_bias_kwargs) \n","        x = BatchNormalization()(x)\n","        activation = kwargs.get('activation')\n","        if not activation: \n","            x = LeakyReLU(alpha=0.1)(x) #默认使用LeakyReLU\n","        else:\n","            print('#使用:', activation )\n","            x = Activation(activation)(x)\n","        return x\n","\n","    @showinfo\n","    def __res_blocks(self, x, filters, num_blocks, name):\n","        \"\"\"\n","        @ A series of resblocks starting with a downsampling Convolution2D\n","        :param\n","            x: input \n","            filters: 卷积核数量\n","            num_blocks: 残差block数量\n","        :return \n","            Output tensor  \n","        \"\"\" \n","        # uses left and top padding instead of 'same' mode\n","        x = ZeroPadding2D(((1,0),(1,0)))(x)\n","        #图片尺寸 缩小一半\n","        x = self.__Conv2D_BN(x, filters, (3,3), strides=(2,2))\n","        #多个残差块\n","        for i in range(num_blocks):\n","            with tf.name_scope(name+str(i)):\n","                y = self.__Conv2D_BN(x, filters//2, (1,1))\n","                y = self.__Conv2D_BN(y, filters, (3,3))\n","                x = Add()([x,y])  #residual shoutcut\n","        return x\n","\n","    \"\"\"\n","    darknet骨干网络\n","    \"\"\"\n","    def darknet_body(self, input):\n","        \"\"\"\n","        @@@ Darknent body having 52 Convolution2D layers\n","        :param\n","            input: input tensor\n","        :return \n","            Output tensor  \n","        \"\"\"         \n","        x = self.__Conv2D_BN(input, 32, (3,3))\n","        x = self.__res_blocks(x, 64, 1, 'Residual_Block_1_')\n","        x = self.__res_blocks(x, 128, 2, 'Residual_Block_2_')\n","        x = self.__res_blocks(x, 256, 8, 'Residual_Block_3_')\n","        x = self.__res_blocks(x, 512, 8, 'Residual_Block_4_')\n","        x = self.__res_blocks(x, 1024, 4, 'Residual_Block_5_')\n","        return x    \n","\n","    \"\"\"\n","    new net 骨干网络\n","    \"\"\"\n","    def newnet_body(self, input):\n","        \"\"\"\n","        @ new\n","        :Args\n","            input: input tensor\n","        :returns \n","            Output tensor  \n","        \"\"\"         \n","        x = self.__Conv2D_BN(input, 16, (3,3))\n","        x = self.__res_blocks(x, 32, 1, 'Residual_Block_1_')\n","        x = self.__res_blocks(x, 64, 2, 'Residual_Block_2_')\n","        x = self.__res_blocks(x, 64, 2, 'Residual_Block_3_')\n","        x = self.__res_blocks(x, 32, 2, 'Residual_Block_3_')\n","        x = self.__res_blocks(x, 16, 1, 'Residual_Block_3_')\n","        return x   \n","\n","    def __make_last_layer(self, input, filters, out_filters):\n","        \"\"\"\n","        @ 6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer\n","        :param\n","            input: input tensor\n","            filters: 卷积核数目\n","            out_filters: y输出卷积核数目（通道数）\n","        :return \n","            x, y: 两个Output tensor  \n","        \"\"\" \n","        x = self.__Conv2D_BN(input, filters, (1,1))\n","        x = self.__Conv2D_BN(x, filters*2, (3,3))\n","        x = self.__Conv2D_BN(x, filters, (1,1))\n","        x = self.__Conv2D_BN(x, filters*2, (3,3))\n","        x = self.__Conv2D_BN(x, filters, (1,1))\n","\n","        y = self.__Conv2D_BN(x, filters*2, (3,3))\n","        y = self.__Conv2D(y, out_filters, (1,1))\n","        return x, y\n","\n","    def fpn(self, net, *args, **kwargs):\n","        \"\"\"\n","        @ feature pyramid networks 特征金字塔网络FPN\n","        #同时利用低层特征高分辨率和高层特征的高语义信息，通过融合这些不同层的特征达到预测的效果。\n","        #并且预测是在每个融合后的特征层上单独进行的\n","        :param\n","            net: 骨干网络model\n","        :return \n","            [Output-tensor,] 效果：所有尺度下的特征都有丰富的语义信息 \n","        \"\"\"  \n","        #以每个‘stage’为一个pyramid level，取每个stage最后layer输出的feature map作为pyramid level\n","        #对应darknet,是stage2 stage3 stage4 stage5的res block各自最后一个输出\n","        #stage2 stage3 stage4 stage5的res block各自最后一个输出\n","        darknet = net  #input[416,416]\n","        C5 = darknet.layers[184].output  #stage5输出 [13,13,1024]\n","        C4 = darknet.layers[152].output  #stage4输出 [26,26,512]\n","        C3 = darknet.layers[92].output  #stage3输出 [52,52,256]\n","        C2 = darknet.layers[32].output  #stage2输出 [104,104,128]\n","\n","        # P2，P3，P4，P5\n","        out_filters = 256  #输出通道数 都一样\n","        P5 = self.__Conv2D(C5, out_filters, (1,1), name='fpn_p5')\n","        P4 = Add()([  #横向连接\n","            UpSampling2D(2)(P5),  #上采样跟C4同尺寸\n","            self.__Conv2D(C4, out_filters, (1,1)),  #1*1卷积跟P5通道数一样\n","            ])\n","        P3 = Add()([   \n","            UpSampling2D(2)(P4),\n","            self.__Conv2D(C3, out_filters, (1,1)),\n","            ])\n","        P2 = Add()([   \n","            UpSampling2D(2)(P3),\n","            self.__Conv2D(C2, out_filters, (1,1)),\n","            ])\n","        # P2-P4最后又做了一次3*3的卷积，作用是消除上采样带来的混叠效应\n","        P4 = self.__Conv2D(P4, out_filters, (3,3), name='fpn_p4')\n","        P3 = self.__Conv2D(P3, out_filters, (3,3), name='fpn_p3')\n","        P2 = self.__Conv2D(P2, out_filters, (3,3), name='fpn_p2')\n","        # 最后得到了融合了不同层级特征的特征图列表\n","        rpn_feature_maps = [P2, P3, P4, P5]\n","        return rpn_feature_maps\n","\n","\n","    def create_model(self):\n","        \"\"\"\n","        @ 创建 模型\n","        :param\n","            None \n","        :return \n","            model\n","        \"\"\"\n","        model_input = Input(shape=X_SHAPE) \n","        #new网络模型\n","        newnet = Model(model_input, self.newnet_body(model_input)) \n","        # darknet.load_weights('lib/darknet53.h5')\n","        # P2, P3, P4, P5 = self.fpn(darknet)\n","        \n","        # for layer in darknet.layers[:]:\n","        #     layer.trainable = False  #darknet部分 权重不训练  \n","        x = self.__Conv2D(newnet.output, 16, (3,3), strides=(2,2), padding='valid')    \n","        # x = self.__Conv2D(x, CLASS_NUM, (7,7), strides=(5,5))  \n","        # x = self.__Conv2D(x, CLASS_NUM, (3,3), strides=(2,2), padding='valid')  \n","        x = Flatten()(x)\n","        # x = Dropout(0.5)(x)\n","        pridictions = Dense(CLASS_NUM, activation='softmax', name='softmax')(x)\n","        # 构建我们需要训练的完整模型\n","        model = Model(model_input, pridictions)\n","        # model.load_weights('backup/weights-17-0.290.hdf5', by_name=True)\n","\n","\n","        # 编译模型（一定要在锁层以后操作）\n","        model.compile(optimizer='adam', #微调时考虑 (lr=1e-4)\n","               loss='categorical_crossentropy',\n","               metrics=['acc'],\n","               )            \n","        return model\n","\n","    def create_model_old1(self):\n","        \"\"\"\n","        @ 创建 模型\n","        :param\n","            None \n","        :return \n","            model\n","        \"\"\"\n","        model_input = Input(shape=X_SHAPE) \n","        #darknet 53成网络模型\n","        darknet = Model(model_input, self.darknet_body(model_input)) \n","        #加载darknet 预训练参数 （待验证）\n","        darknet.load_weights('lib/darknet53.h5')\n","        for layer in darknet.layers[:-4]:\n","            layer.trainable = False  #darknet部分 权重不训练\n","        # print(darknet.summary())\n","\n","        P2, P3, P4, P5 = self.fpn(darknet)\n","        # print(P2, P3, P4, P5)\n","\n","        # x = self.__Conv2D_BN(P4, 128, (1,1))\n","        # x = self.__Conv2D_BN(x, 256, (3,3))\n","        # x = self.__Conv2D_BN(x, 128, (1,1))\n","        x = self.__res_blocks(P4, 128, 1, name='res')\n","        x = Dropout(0.3)(x)\n","        # x = AveragePooling2D()(x)\n","        x = Flatten()(x)\n","\n","        pridictions = Dense(CLASS_NUM, activation='softmax')(x)\n","        # 构建我们需要训练的完整模型\n","        model = Model(model_input, pridictions)\n","        # model.load_weights('backup/weights-05-0.300.hdf5', by_name=True)\n","\n","\n","        # 编译模型（一定要在锁层以后操作）\n","        model.compile(optimizer=keras.optimizers.Adam(learning_rate=e-3),\n","               loss='categorical_crossentropy',\n","               metrics=['acc'],\n","               )            \n","        return model\n","\n","    def fit(self, xs, vs):\n","        \"\"\"\n","        @@@ model fit\n","        :param\n","            xs: 训练集\n","            vs: 验证集\n","        :return \n","            history 对象。其 History.history 属性是连续 epoch 训练损失和评估值，以及验证集损失和评估值的记录（如果适用）\n","        \"\"\"    \n","        # 当评价指标不在提升时，减少学习率\n","        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)  \n","        #停止阈值min_delta和patience需要相互配合，避免模型停止在抖动的过程中。min_delta降低，patience减少；而min_delta增加，则patience增加。\n","        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n","        #检查点\n","        checkpoint = ModelCheckpoint(filepath= self.cp_path,\n","                        verbose= 1,\n","                        save_weights_only = True,\n","                        mode = 'max',\n","                        load_weights_on_restart = True,\n","                        )\n","        hist = self.model.fit(xs,\n","                    epochs= EPOCHS,\n","                    verbose= 1,\n","                    steps_per_epoch= STEP_PER_EPOCH['xs'],\n","                    validation_data= vs,\n","                    validation_steps= STEP_PER_EPOCH['vs'],\n","                    callbacks= [early_stopping, checkpoint, reduce_lr],  \n","                    )\n","        self.model.save_weights('my_model_weights.h5') #最后保存一次\n","        return hist\n","\n","def _show_weights(id):\n","    \"\"\"@显示weights\n","    :param\n","        id: 第几层        \n","    \"\"\"\n","    sess = tf.Session()\n","    sess.run(tf.global_variables_initializer())\n","    weights = m.model.layers[id].weights\n","    print(sess.run(weights[0][...,:3]))\n","        \n","def _show_h5():\n","    import h5py\n","    f = h5py.File('lib/darknet53.h5', 'r') \n","    # it = f['/model_weights/add_1'] \n","    # it = f.get('/model_weights/batch_normalization_1/batch_normalization_1') \n","    it = f.get('/model_weights/conv2d_1/conv2d_1').get('kernel:0')\n","    # [i for i in it.items()]\n","    # for i in ['beta:0', 'gamma:0', 'moving_mean:0', 'moving_variance:0']:\n","    #     print(it.get(i).shape, it.get(i).value)\n","    print(it.value[...,:3])\n","    \n","def show_h5_list():\n","    \"\"\"@显示h5文件的内容\"\"\"\n","    #遍历文件中的一级组\n","    print(list([key for key in f.keys()]))\n","    for group in f.attrs():\n","        # print (group)\n","        #根据一级组名获得其下面的组\n","        group_read = f[group]\n","        #遍历该一级组下面的子组\n","        for subgroup in [k for k in group_read.keys()]:\n","            print(subgroup)\n","            #根据一级组和二级组名获取其下面的dataset          \n","            dset_read = f[group+'/'+subgroup] \n","            #获取dataset数据\n","            #遍历该子组下所有的dataset\n","            for dset in dset_read.keys():\n","                _dataset = f[group+'/'+subgroup+'/'+dset]\n","                print(_dataset.name)\n","                data = np.array(_dataset)\n","                print(data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KodN7Pbc5ZCW","colab_type":"code","outputId":"bc98d5c9-966f-40ee-a42e-9784b8b9ff92","colab":{"base_uri":"https://localhost:8080/","height":352}},"source":["dg = DatasetGenerator()\n","#     print(STEP_PER_EPOCH)\n","\n","XS_dataset = dg.read_data_from_TFRecoard('xs') #单label训练集\n","VS_dataset = dg.read_data_from_TFRecoard('vs')\n","\n","m = MyModel()\n","m.fit(XS_dataset, VS_dataset)\n","\n","# img, label = dg.iterator_from_tfrecoard('xs') #测试用\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["对【\"xs\"单label训练集】的4个TFRecoard，检查完毕！\n","对【\"vs\"单label验证集】的2个TFRecoard，检查完毕！\n","对【\"xm\"多label训练集】的1个TFRecoard，检查完毕！\n","对【\"vm\"多label验证集】的1个TFRecoard，检查完毕！\n","OK! 【['xs', 'vs', 'xm', 'vm']】的所有TFRecoard文件检查完毕！\n","\n","从【lib/xs_1.tfrecord...】读取4个TFRecoard...\n","WARNING:tensorflow:Entity <bound method DatasetGenerator.__parse_function of <__main__.DatasetGenerator object at 0x7fc470897c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Mangled names are not yet supported by AutoGraph\n","WARNING: Entity <bound method DatasetGenerator.__parse_function of <__main__.DatasetGenerator object at 0x7fc470897c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Mangled names are not yet supported by AutoGraph\n","\n","【read_data_from_TFRecoard】xs cost 0.06S\n","从【lib/vs_1.tfrecord...】读取2个TFRecoard...\n","\n","【read_data_from_TFRecoard】vs cost 0.03S\n","WARNING:tensorflow:`load_weights_on_restart` argument is deprecated. Please use `model.load_weights()` for loading weights before the start of `model.fit()`.\n","Train on 224 steps, validate on 96 steps\n","Epoch 1/50\n"," 31/224 [===>..........................] - ETA: 4:42 - loss: 1.9897 - acc: 0.4294"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mir4HAUsz2Ia","colab_type":"code","colab":{}},"source":["m = MyModel()\n","m.model.summary()\n","# for i, layer in enumerate(m.model.layers):\n","#     print(i, layer)\n","# plot_model(model.model, to_file='model.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vrfJ_O8JRKzk","colab_type":"code","colab":{}},"source":["# t = m.model.evaluate(VS_dataset, verbose=1, steps=STEP_PER_EPOCH['vs'],)\n","\n","# dg._test_read_write()\n","# dg._test_resize()\n","#     t = dg._separete(Pic.get_train_NLs('single'))\n","\n","\n","#     #比较resize前后的图片\n","#     df = dg._test_get_img('xs', 15)\n","#     before, after = [], []\n","#     for index, (name, label) in df.iterrows():\n","#         img = Pic.read_image(name)\n","#         before.append(img)\n","#         img = Pic._resize(img)\n","#         after.append(img)\n","#     Pic.show_double_img(before, after, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-NgnQK2rph2","colab_type":"code","colab":{}},"source":["m.model.layers[-7].weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTWlfg1fThaX","colab_type":"code","colab":{}},"source":["m.model.save('backup/model.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAEMET1MdQsS","colab_type":"code","colab":{}},"source":["m.model.save_weights('my_model_weights19.h5')\n","# m.model.load_weights('my_model_weights.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-Eo4W6Vhtqo","colab_type":"code","colab":{}},"source":["*a = (1, 'dd', (2,3))\n","b,c,d = a\n","type(a)"],"execution_count":0,"outputs":[]}]}